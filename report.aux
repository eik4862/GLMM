\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\bibstyle{genetics}
\HyPL@Entry{0<</S/D>>}
\babel@aux{english}{}
\citation{bohorquez2008generation}
\citation{galambos198140}
\citation{rickards1982steady,picton1987potentials,kuwada1986scalp,cohen1991comparison}
\citation{john2000master}
\citation{cone2002auditory}
\citation{pinheiro2006mixed,agresti2000random}
\citation{demidenko2013mixed}
\@writefile{toc}{\contentsline {section}{서론}{2}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{모형 적합}{2}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{GLMM 모형}{2}{section*.1}\protected@file@percent }
\citation{searle2001generalized}
\newlabel{eq:glmm_model}{{1}{3}{GLMM 모형}{equation.0.1}{}}
\newlabel{eq:hierachy_model}{{2}{3}{GLMM 모형}{equation.0.2}{}}
\citation{capanu2013assessment}
\citation{tuerlinckx2006statistical}
\citation{zhang2008variance}
\citation{mcculloch2011prediction}
\citation{shoker2005artifact,nolan2010faster,lawhern2013detect,mognon2011adjust}
\citation{subha2010eeg}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \bf  이상이 있는 것으로 판단되어 분석에서 제외된 채널\relax }}{6}{table.caption.2}\protected@file@percent }
\@cons\caption@pkg@list{{ragged2e}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:bad_chan}{{1}{6}{\bf 이상이 있는 것으로 판단되어 분석에서 제외된 채널\relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{자료 전처리}{6}{equation.0.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{이상 채널 제거}{6}{equation.0.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{LFP 정규화}{6}{figure.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Bandpower 계산}{6}{figure.caption.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip AC\textsubscript  {L}채널에서 측정된 LFP의 시계열도. 각 피실험체 별로 자극이 시작된 순간을 0초로 하여 세션 1의 첫 10번의 시행에서 관측한 자료를 겹쳐서 나타내었다. 빨간 점선은 prestimulus, stimulus, poststimulus 구간을 나타낸다. 육안으로도 쉽게 1번과 7번 피실험체의 AC\textsubscript  {L}채널 전극에 이상이 있음을 명백히 알 수 있다. 한편, 자극이 시작되는 0초 부근에서 펄스 형태의 뇌파가 지속적으로 관측된다는 점도 확인할 수 있다.\relax }}{7}{figure.caption.3}\protected@file@percent }
\newlabel{fig:LFP_raw}{{1}{7}{\small \ACL 채널에서 측정된 LFP의 시계열도. 각 피실험체 별로 자극이 시작된 순간을 0초로 하여 세션 1의 첫 10번의 시행에서 관측한 자료를 겹쳐서 나타내었다. 빨간 점선은 prestimulus, stimulus, poststimulus 구간을 나타낸다. 육안으로도 쉽게 1번과 7번 피실험체의 \ACL 채널 전극에 이상이 있음을 명백히 알 수 있다. 한편, 자극이 시작되는 0초 부근에서 펄스 형태의 뇌파가 지속적으로 관측된다는 점도 확인할 수 있다.\relax }{figure.caption.3}{}}
\citation{miller2012probability}
\citation{stoica2005spectral}
\citation{ehlers1997slow,vertes2005hippocampal,baker2007oscillatory,crick1990towards}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {신경과학계에서 흔히 분석하는 주파수대}\relax }}{8}{table.caption.4}\protected@file@percent }
\newlabel{tab:band_freq}{{2}{8}{\textbf {신경과학계에서 흔히 분석하는 주파수대}\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{탐색적 자료분석}{8}{table.caption.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip 로그 변환 \textbf  {A} 전과 \textbf  {B} 후 AC\textsubscript  {L}채널 delta band의 bandpower로 그린 box plot. 그 중에서 2번 피실험체의 로그 변환 \textbf  {C} 전과 \textbf  {D} 후의 추정된 분포. 추정된 분포는 Gaussian 커널을 이용한 커널분포추정의 결과이며 이때의 bandwidth는 $0.9n^{-1/5}\qopname  \relax m{min}\{\setbox \z@ \hbox {\mathsurround \z@ $\textstyle \sigma $}\mathaccent "0362{\sigma },\tmspace  +\thinmuskip {.1667em}\mathrm  {IQR}/1.34\}$로 추정하였다 (로그 변환 전 Hit: 0.0609, Miss: 0.1143, 로그 변환 후 Hit: 0.1789, Miss: 0.2590). 여기서 $n$은 표본의 수이며 $\setbox \z@ \hbox {\mathsurround \z@ $\textstyle \sigma $}\mathaccent "0362{\sigma }$는 표본표준편차이다. \textbf  {E} 피실험체별 {Hit} 그룹과 {Miss} 그룹 간의 비율. 피실험체에 따라 {Hit}의 비율이 {Miss} 비율보다 작게는 1.5배(피실험체 7)에서 많게는 10.4배(피실험체 5) 가량 더 많았다. \textbf  {F}$\sim $\textbf  {H} 1번 피실험체의 3개의 주성분을 추출하여 그린 산점도. 어느 주성분 방향에서 보나 두 그룹이 서로 거의 구분이 되지 않고 잘 섞여 있음을 알 수 있다. 주성분 분석에는 R 함수 \textrm  {\textit  {prcomp}}이 사용되었으며 피실험체별로 이상 채널을 제외한 모든 채널에서의 bandpower를 하나의 변량으로 하여 주성분분석을 수행하였다. 이때, 각 변량의 단위가 모두 같은 것을 고려하여 scaling은 하지 않았다.\relax }}{9}{figure.caption.5}\protected@file@percent }
\newlabel{fig:EDA}{{2}{9}{\small 로그 변환 \textbf {A} 전과 \textbf {B} 후 \ACL 채널 delta band의 bandpower로 그린 box plot. 그 중에서 2번 피실험체의 로그 변환 \textbf {C} 전과 \textbf {D} 후의 추정된 분포. 추정된 분포는 Gaussian 커널을 이용한 커널분포추정의 결과이며 이때의 bandwidth는 $0.9n^{-1/5}\min \{\widehat {\sigma },\,\mathrm {IQR}/1.34\}$로 추정하였다 (로그 변환 전 Hit: 0.0609, Miss: 0.1143, 로그 변환 후 Hit: 0.1789, Miss: 0.2590). 여기서 $n$은 표본의 수이며 $\widehat {\sigma }$는 표본표준편차이다. \textbf {E} 피실험체별 {Hit} 그룹과 {Miss} 그룹 간의 비율. 피실험체에 따라 {Hit}의 비율이 {Miss} 비율보다 작게는 1.5배(피실험체 7)에서 많게는 10.4배(피실험체 5) 가량 더 많았다. \textbf {F}$\sim $\textbf {H} 1번 피실험체의 3개의 주성분을 추출하여 그린 산점도. 어느 주성분 방향에서 보나 두 그룹이 서로 거의 구분이 되지 않고 잘 섞여 있음을 알 수 있다. 주성분 분석에는 R 함수 \textrm {\textit {prcomp}}이 사용되었으며 피실험체별로 이상 채널을 제외한 모든 채널에서의 bandpower를 하나의 변량으로 하여 주성분분석을 수행하였다. 이때, 각 변량의 단위가 모두 같은 것을 고려하여 scaling은 하지 않았다.\relax }{figure.caption.5}{}}
\citation{friedman2010regularization}
\@writefile{toc}{\contentsline {subsection}{GLMM 모형 적합}{10}{figure.caption.5}\protected@file@percent }
\newlabel{eq:glmm_example}{{17}{10}{GLMM 모형 적합}{equation.0.17}{}}
\citation{chicco2020advantages}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \bf  Elatsic net으로 선별된 설명변수\relax }}{11}{table.caption.6}\protected@file@percent }
\newlabel{tab:e_net_select}{{3}{11}{\bf Elatsic net으로 선별된 설명변수\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{CV 및 시뮬레이션}{11}{figure.caption.7}\protected@file@percent }
\citation{beyer1999nearest}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Elastic net의 hyperparameter tuning 결과. $\alpha =0.01i$ ($i=1,\tmspace  +\thinmuskip {.1667em}\cdots  ,\tmspace  +\thinmuskip {.1667em}99$)와 $\lambda =e^{-1-0.04i}$ ($i=0,\tmspace  +\thinmuskip {.1667em}\cdots  ,\tmspace  +\thinmuskip {.1667em}100$)에 대해 grid search를 수행하였다. 각 조합에 대해 5번의 10-fold CV를 통해 계산된 \textbf  {A} AUC와 \textbf  {B} 선별된 변수의 수. AUC의 최댓값은 $\alpha =0.04,\tmspace  +\thinmuskip {.1667em}\lambda =0.0089$ ($\mathrm  {AUC}=0.7340$, 19개의 변수 모두 사용)에서 얻어졌으며 (검은색 +), 이에 1sd rule을 적용하여 얻은 sparse한 모형은 $\alpha =0.15,\tmspace  +\thinmuskip {.1667em}\lambda =0.0118$ ($\mathrm  {AUC}=0.7332$, 17개의 변수 사용)에서 얻어졌다 (빨간색 +). 피실험체를 indicator variable로 하여 elastic net을 적합하였으며 그 적합에는 \texttt  {R} 함수 \textrm  {\textit  {glmnet}}(glmnet 패키지)을 사용하였다.\relax }}{12}{figure.caption.7}\protected@file@percent }
\newlabel{fig:e_net_tuning}{{3}{12}{\small Elastic net의 hyperparameter tuning 결과. $\alpha =0.01i$ ($i=1,\,\cdots ,\,99$)와 $\lambda =e^{-1-0.04i}$ ($i=0,\,\cdots ,\,100$)에 대해 grid search를 수행하였다. 각 조합에 대해 5번의 10-fold CV를 통해 계산된 \textbf {A} AUC와 \textbf {B} 선별된 변수의 수. AUC의 최댓값은 $\alpha =0.04,\,\lambda =0.0089$ ($\mathrm {AUC}=0.7340$, 19개의 변수 모두 사용)에서 얻어졌으며 (검은색 +), 이에 1sd rule을 적용하여 얻은 sparse한 모형은 $\alpha =0.15,\,\lambda =0.0118$ ($\mathrm {AUC}=0.7332$, 17개의 변수 사용)에서 얻어졌다 (빨간색 +). 피실험체를 indicator variable로 하여 elastic net을 적합하였으며 그 적합에는 \texttt {R} 함수 \textrm {\textit {glmnet}}(glmnet 패키지)을 사용하였다.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Naive Bayes}{12}{figure.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{LDA}{12}{figure.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{KNN}{12}{figure.caption.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \textbf  {A} KNN의 hyperparameter tuning 결과. 이웃의 수 $k=1,\tmspace  +\thinmuskip {.1667em}\cdots  ,\tmspace  +\thinmuskip {.1667em}15$와 주성분의 개수 $d=1,\tmspace  +\thinmuskip {.1667em}\cdots  ,\tmspace  +\thinmuskip {.1667em}15$에 대해 grid search를 수행하였으며 KNN은 각각의 피실험체에 대해 적합하였다. 5번의 10-fold CV를 통해 계산된 AUC의 최댓값은 $k=9$과 $d=10$인 경우($\mathrm  {AUC}=0.752$)에 얻어졌다 (검은색 +). \textbf  {B} ANN의 hyperparameter tuning 결과. 노드의 개수 $n=1,\tmspace  +\thinmuskip {.1667em}\cdots  ,\tmspace  +\thinmuskip {.1667em}25$과 decay constant $d=e^{1-0.25i}$ ($i=0,\tmspace  +\thinmuskip {.1667em}\cdots  ,\tmspace  +\thinmuskip {.1667em}24$)에 대해 grid search를 수행하으며 피실험체를 indicator variable로 하여 ANN을 적합하였다. 5번의 10-fold CV를 통해 계산된 AUC의 최댓값은 $n=23$과 $d=e^{-0.25}$인 경우($\mathrm  {AUC}=0.775$)에 얻어졌다 (검은색 +). \textbf  {C} 로지스틱 회귀의 link tuning 결과. Logit, probit, cauchit, complementary log-log(cloglog)에 대해 피실험체를 indicator variable로 하여 로지스틱 회귀모형을 적합하였다. 최초의 full model은 AIC를 기준으로 stepwise 모형 선택을 거쳤으며, 그 결과 선택된 모형으로 5번의 10-fold CV를 수행하고 AUC를 구하였다. AUC의 최댓값은 logistic link를 사용한 경우($\mathrm  {AUC}=0.751$)에 얻어졌다. \textbf  {D} Random forest의 hyperparameter tuning 결과. 개별 결정트리 적합시 사용할 변수의 수 $d=1,\tmspace  +\thinmuskip {.1667em}\cdots  ,\tmspace  +\thinmuskip {.1667em}21$에 대해 grid search를 수행하였다. 5번의 10-fold CV를 통해 계산된 AUC의 최댓값은 $d=2$인 경우($\mathrm  {AUC}=0.755$)에 얻어졌다 (빨간색 네모).\relax }}{13}{figure.caption.8}\protected@file@percent }
\newlabel{fig:tune}{{4}{13}{\small \textbf {A} KNN의 hyperparameter tuning 결과. 이웃의 수 $k=1,\,\cdots ,\,15$와 주성분의 개수 $d=1,\,\cdots ,\,15$에 대해 grid search를 수행하였으며 KNN은 각각의 피실험체에 대해 적합하였다. 5번의 10-fold CV를 통해 계산된 AUC의 최댓값은 $k=9$과 $d=10$인 경우($\mathrm {AUC}=0.752$)에 얻어졌다 (검은색 +). \textbf {B} ANN의 hyperparameter tuning 결과. 노드의 개수 $n=1,\,\cdots ,\,25$과 decay constant $d=e^{1-0.25i}$ ($i=0,\,\cdots ,\,24$)에 대해 grid search를 수행하으며 피실험체를 indicator variable로 하여 ANN을 적합하였다. 5번의 10-fold CV를 통해 계산된 AUC의 최댓값은 $n=23$과 $d=e^{-0.25}$인 경우($\mathrm {AUC}=0.775$)에 얻어졌다 (검은색 +). \textbf {C} 로지스틱 회귀의 link tuning 결과. Logit, probit, cauchit, complementary log-log(cloglog)에 대해 피실험체를 indicator variable로 하여 로지스틱 회귀모형을 적합하였다. 최초의 full model은 AIC를 기준으로 stepwise 모형 선택을 거쳤으며, 그 결과 선택된 모형으로 5번의 10-fold CV를 수행하고 AUC를 구하였다. AUC의 최댓값은 logistic link를 사용한 경우($\mathrm {AUC}=0.751$)에 얻어졌다. \textbf {D} Random forest의 hyperparameter tuning 결과. 개별 결정트리 적합시 사용할 변수의 수 $d=1,\,\cdots ,\,21$에 대해 grid search를 수행하였다. 5번의 10-fold CV를 통해 계산된 AUC의 최댓값은 $d=2$인 경우($\mathrm {AUC}=0.755$)에 얻어졌다 (빨간색 네모).\relax }{figure.caption.8}{}}
\citation{leshno1993multilayer}
\@writefile{toc}{\contentsline {paragraph}{ANN}{14}{figure.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{로지스틱 회귀}{14}{figure.caption.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \bf  로지스틱 회귀에서의 link 함수\relax }}{15}{table.caption.9}\protected@file@percent }
\newlabel{tab:logistic_link}{{4}{15}{\bf 로지스틱 회귀에서의 link 함수\relax }{table.caption.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Random forest}{15}{table.caption.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{결과}{15}{equation.0.20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip GLMM, naive Bayes, LDA, KNN, ANN, 로지스틱 회귀, random forest의 CV 결과. 100번의 10-fold CV를 통해 계산한 \textbf  {A} AUC, \textbf  {B} MCC, \textbf  {C} F score, \textbf  {D} ACC, \textbf  {E} TPR, \textbf  {F} TNR을 나타냈다. 에러바는 1sd를 의미한다. GLMM, ANN, 로지스틱 회귀, random forest는 피실험체를 indicator variable로 하여 하나의 모형을 적합했고, naive Bayes, LDA, KNN는 피실험체별로 하나씩의 모형을 적합했다.\relax }}{16}{figure.caption.10}\protected@file@percent }
\newlabel{fig:cv}{{5}{16}{\small GLMM, naive Bayes, LDA, KNN, ANN, 로지스틱 회귀, random forest의 CV 결과. 100번의 10-fold CV를 통해 계산한 \textbf {A} AUC, \textbf {B} MCC, \textbf {C} F score, \textbf {D} ACC, \textbf {E} TPR, \textbf {F} TNR을 나타냈다. 에러바는 1sd를 의미한다. GLMM, ANN, 로지스틱 회귀, random forest는 피실험체를 indicator variable로 하여 하나의 모형을 적합했고, naive Bayes, LDA, KNN는 피실험체별로 하나씩의 모형을 적합했다.\relax }{figure.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces \bf  GLMM 적합 결과 (고정효과)\relax }}{17}{table.caption.11}\protected@file@percent }
\newlabel{tb:glmm_fixed}{{5}{17}{\bf GLMM 적합 결과 (고정효과)\relax }{table.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces \bf  GLMM 적합 결과 (랜덤효과)\relax }}{17}{table.caption.12}\protected@file@percent }
\newlabel{tb:glmm_rand}{{6}{17}{\bf GLMM 적합 결과 (랜덤효과)\relax }{table.caption.12}{}}
\citation{han2019functional}
\@writefile{toc}{\contentsline {section}{코드 및 분석도구}{18}{table.caption.12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Acknowledgement}{18}{table.caption.12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Technicality}{18}{table.caption.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{라플라스 근사를 이용한 GLMM의 로그가능도 근사}{18}{section*.13}\protected@file@percent }
\newlabel{eq:GLMMloglik}{{21}{18}{라플라스 근사를 이용한 GLMM의 로그가능도 근사}{equation.0.21}{}}
\newlabel{eq:limsup}{{26}{19}{라플라스 근사를 이용한 GLMM의 로그가능도 근사}{equation.0.26}{}}
\newlabel{eq:liminf}{{30}{20}{라플라스 근사를 이용한 GLMM의 로그가능도 근사}{equation.0.30}{}}
\@writefile{toc}{\contentsline {subsection}{GLMM의 고정효과에 대한 검정통계량의 극한분포}{20}{equation.0.35}\protected@file@percent }
\newlabel{eq:linearHypo}{{36}{20}{GLMM의 고정효과에 대한 검정통계량의 극한분포}{equation.0.36}{}}
\citation{van2000asymptotic}
\newlabel{eq:uniformConv}{{44}{21}{GLMM의 고정효과에 대한 검정통계량의 극한분포}{equation.0.44}{}}
\citation{van2000asymptotic}
\citation{van2000asymptotic}
\citation{bickel2015mathematical}
\@writefile{toc}{\contentsline {subsection}{GLMM의 랜덤효과에 대한 예측값}{22}{equation.0.47}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{이진 분류 문제와 CV}{23}{equation.0.47}\protected@file@percent }
\newlabel{eq:tprEst}{{53}{24}{이진 분류 문제와 CV}{equation.0.53}{}}
\@writefile{toc}{\contentsline {subsection}{베이즈 분류기}{25}{equation.0.59}\protected@file@percent }
\newlabel{eq:Bayes}{{60}{25}{베이즈 분류기}{equation.0.60}{}}
\citation{fisher1936use}
\newlabel{eq:KNN}{{66}{27}{베이즈 분류기}{equation.0.66}{}}
\citation{Cybenko1989}
\citation{hanin2017approximating}
\citation{kidger2019universal}
\citation{lin2006random}
\bibdata{refs}
\bibcite{agresti2000random}{{1}{2000}{{Agresti {\em  et~al.\/}}}{{Agresti, Booth*, Hobert*, {\rm  \&} Caffo*}}}
\bibcite{baker2007oscillatory}{{2}{2007}{{Baker}}{{}}}
\bibcite{beyer1999nearest}{{3}{1999}{{Beyer {\em  et~al.\/}}}{{Beyer, Goldstein, Ramakrishnan, {\rm  \&} Shaft}}}
\bibcite{bickel2015mathematical}{{4}{2015}{{Bickel {\rm  \&} Doksum}}{{}}}
\bibcite{bohorquez2008generation}{{5}{2008}{{Boh{\'o}rquez {\rm  \&} {\"O}zdamar}}{{}}}
\bibcite{capanu2013assessment}{{6}{2013}{{Capanu {\em  et~al.\/}}}{{Capanu, G{\"o}nen, {\rm  \&} Begg}}}
\bibcite{chicco2020advantages}{{7}{2020}{{Chicco {\rm  \&} Jurman}}{{}}}
\bibcite{cohen1991comparison}{{8}{1991}{{Cohen {\em  et~al.\/}}}{{Cohen, Rickards, {\rm  \&} Clark}}}
\bibcite{cone2002auditory}{{9}{2002}{{Cone-Wesson {\em  et~al.\/}}}{{Cone-Wesson, Dowell, Tomlin, Rance, {\rm  \&} Ming}}}
\bibcite{crick1990towards}{{10}{1990}{{Crick {\rm  \&} Koch}}{{}}}
\bibcite{Cybenko1989}{{11}{1989}{{Cybenko}}{{}}}
\bibcite{demidenko2013mixed}{{12}{2013}{{Demidenko}}{{}}}
\bibcite{ehlers1997slow}{{13}{1997}{{Ehlers {\rm  \&} Kupfer}}{{}}}
\@writefile{toc}{\contentsline {section}{참고문헌}{29}{section.1}\protected@file@percent }
\bibcite{fisher1936use}{{14}{1936}{{Fisher}}{{}}}
\bibcite{friedman2010regularization}{{15}{2010}{{Friedman {\em  et~al.\/}}}{{Friedman, Hastie, {\rm  \&} Tibshirani}}}
\bibcite{galambos198140}{{16}{1981}{{Galambos {\em  et~al.\/}}}{{Galambos, Makeig, {\rm  \&} Talmachoff}}}
\bibcite{han2019functional}{{17}{2019}{{Han {\em  et~al.\/}}}{{Han, Lee, {\rm  \&} Choi}}}
\bibcite{hanin2017approximating}{{18}{2017}{{Hanin {\rm  \&} Sellke}}{{}}}
\bibcite{john2000master}{{19}{2000}{{John {\rm  \&} Picton}}{{}}}
\bibcite{kidger2019universal}{{20}{2019}{{Kidger {\rm  \&} Lyons}}{{}}}
\bibcite{kuwada1986scalp}{{21}{1986}{{Kuwada {\em  et~al.\/}}}{{Kuwada, Batra, {\rm  \&} Maher}}}
\bibcite{lawhern2013detect}{{22}{2013}{{Lawhern {\em  et~al.\/}}}{{Lawhern, Hairston, {\rm  \&} Robbins}}}
\bibcite{leshno1993multilayer}{{23}{1993}{{Leshno {\em  et~al.\/}}}{{Leshno, Lin, Pinkus, {\rm  \&} Schocken}}}
\bibcite{lin2006random}{{24}{2006}{{Lin {\rm  \&} Jeon}}{{}}}
\bibcite{mcculloch2011prediction}{{25}{2011}{{McCulloch {\rm  \&} Neuhaus}}{{}}}
\bibcite{miller2012probability}{{26}{2012}{{Miller {\rm  \&} Childers}}{{}}}
\bibcite{mognon2011adjust}{{27}{2011}{{Mognon {\em  et~al.\/}}}{{Mognon, Jovicich, Bruzzone, {\rm  \&} Buiatti}}}
\bibcite{nolan2010faster}{{28}{2010}{{Nolan {\em  et~al.\/}}}{{Nolan, Whelan, {\rm  \&} Reilly}}}
\bibcite{picton1987potentials}{{29}{1987}{{Picton {\em  et~al.\/}}}{{Picton, Skinner, Champagne, Kellett, {\rm  \&} Maiste}}}
\bibcite{pinheiro2006mixed}{{30}{2006}{{Pinheiro {\rm  \&} Bates}}{{}}}
\bibcite{rickards1982steady}{{31}{1982}{{Rickards {\rm  \&} Clark}}{{}}}
\bibcite{searle2001generalized}{{32}{2001}{{Searle {\rm  \&} McCulloch}}{{}}}
\bibcite{shoker2005artifact}{{33}{2005}{{Shoker {\em  et~al.\/}}}{{Shoker, Sanei, {\rm  \&} Chambers}}}
\bibcite{stoica2005spectral}{{34}{2005}{{Stoica {\em  et~al.\/}}}{{Stoica, Moses, {\em  et~al.\/}}}}
\bibcite{subha2010eeg}{{35}{2010}{{Subha {\em  et~al.\/}}}{{Subha, Joseph, Acharya, {\rm  \&} Lim}}}
\bibcite{tuerlinckx2006statistical}{{36}{2006}{{Tuerlinckx {\em  et~al.\/}}}{{Tuerlinckx, Rijmen, Verbeke, {\rm  \&} De~Boeck}}}
\bibcite{van2000asymptotic}{{37}{2000}{{Van~der Vaart}}{{}}}
\bibcite{vertes2005hippocampal}{{38}{2005}{{Vertes}}{{}}}
\bibcite{zhang2008variance}{{39}{2008}{{Zhang {\rm  \&} Lin}}{{}}}
\newlabel{LastPage}{{}{31}{}{page.31}{}}
\xdef\lastpage@lastpage{31}
\xdef\lastpage@lastpageHy{31}
